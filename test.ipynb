{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wangyichao/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_size=300\n",
    "# is_training=input('train?')\n",
    "# TRAINING= True if is_training=='y' else False\n",
    "TRAINING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs={'8872':'192.168.122.117'\n",
    "          ,'8802':'192.168.122.117','8873':'192.168.122.67','8803':'192.168.122.67',\n",
    "         '8874':'192.168.122.113','8804':'192.168.122.113','8875':'192.168.122.120',\n",
    "        '8876':'192.168.122.30','8877':'192.168.122.208','8878':'192.168.122.58'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "\n",
    "for name in all_runs:\n",
    "    f = open('../dataset/%s_tordata300.pickle'%name,'rb')\n",
    "    dataset+=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    \n",
    "    \n",
    "    len_tr=len(dataset)\n",
    "    train_ratio=float(len_tr-6000)/float(len_tr)\n",
    "    rr= list(range(len(dataset)))\n",
    "    np.random.shuffle(rr)\n",
    "\n",
    "    train_index=rr[:int(len_tr*train_ratio)]\n",
    "    test_index= rr[int(len_tr*train_ratio):] #range(len(dataset_test)) # #\n",
    "    pickle.dump(test_index,open('test_index300.pickle','wb'))\n",
    "else:\n",
    "    test_index=pickle.load(open('test_index300.pickle'))[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negetive_samples=199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(dataset,train_index,test_index,flow_size):\n",
    "    \n",
    "\n",
    "\n",
    "    global negetive_samples\n",
    "\n",
    "\n",
    "\n",
    "    all_samples=len(train_index) #训练集样本个数\n",
    "    labels=np.zeros((all_samples*(negetive_samples+1),1)) # 200个标签数量\n",
    "    l2s=np.zeros((all_samples*(negetive_samples+1),8,flow_size,1)) #？？？？\n",
    "\n",
    "    index=0\n",
    "    random_ordering=[]+train_index #拷贝\n",
    "    for i in tqdm.tqdm( train_index):\n",
    "        #[]#list(lsh.find_k_nearest_neighbors((Y_train[i]/ np.linalg.norm(Y_train[i])).astype(np.float64),(50)))\n",
    "\n",
    "        l2s[index,0,:,0]=np.array(dataset[i]['here'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "        l2s[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s[index,3,:,0]=np.array(dataset[i]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "        l2s[index,4,:,0]=np.array(dataset[i]['here'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "        l2s[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s[index,7,:,0]=np.array(dataset[i]['here'][1]['->'][:flow_size])/1000.0\n",
    "\n",
    "\n",
    "        # if index % (negetive_samples+1) !=0:\n",
    "        #     print(index , len(nears))\n",
    "        #     raise\n",
    "        labels[index,0]=1 #相关的标记为1\n",
    "        m=0\n",
    "        index+=1\n",
    "        # 接下来对负样本进行处理\n",
    "        np.random.shuffle(random_ordering)\n",
    "        for idx in random_ordering:\n",
    "            if idx==i or m>(negetive_samples-1):\n",
    "                continue\n",
    "\n",
    "            m+=1\n",
    "\n",
    "            l2s[index,0,:,0]=np.array(dataset[idx]['here'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "            l2s[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s[index,3,:,0]=np.array(dataset[idx]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "            l2s[index,4,:,0]=np.array(dataset[idx]['here'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "            l2s[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s[index,7,:,0]=np.array(dataset[idx]['here'][1]['->'][:flow_size])/1000.0\n",
    "\n",
    "            #l2s[index,0,:,0]=Y_train[i]#np.concatenate((Y_train[i],X_train[idx]))#(Y_train[i]*X_train[idx])/(np.linalg.norm(Y_train[i])*np.linalg.norm(X_train[idx]))\n",
    "            #l2s[index,1,:,0]=X_train[idx]\n",
    "\n",
    "\n",
    "\n",
    "            labels[index,0]=0\n",
    "            index+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #lsh.setup((X_test / np.linalg.norm(X_test,axis=1,keepdims=True)) .astype(np.float64))\n",
    "    index_hard=0\n",
    "    num_hard_test=0\n",
    "    l2s_test=np.zeros((len(test_index)*(negetive_samples+1),8,flow_size,1))\n",
    "    labels_test=np.zeros((len(test_index)*(negetive_samples+1)))\n",
    "    l2s_test_hard=np.zeros((num_hard_test*num_hard_test,2,flow_size,1))\n",
    "    index=0\n",
    "    random_test=[]+test_index\n",
    "\n",
    "    for i in tqdm.tqdm(test_index):\n",
    "        #list(lsh.find_k_nearest_neighbors((Y_test[i]/ np.linalg.norm(Y_test[i])).astype(np.float64),(50)))\n",
    "\n",
    "\n",
    "\n",
    "        # if index % (negetive_samples+1) !=0:\n",
    "        #     print(index, nears)\n",
    "        #     raise \n",
    "        m=0\n",
    "\n",
    "        np.random.shuffle(random_test)\n",
    "        for idx in random_test:\n",
    "            if idx==i or m>(negetive_samples-1):\n",
    "                continue\n",
    "\n",
    "            m+=1\n",
    "            l2s_test[index,0,:,0]=np.array(dataset[idx]['here'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s_test[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "            l2s_test[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s_test[index,3,:,0]=np.array(dataset[idx]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "            l2s_test[index,4,:,0]=np.array(dataset[idx]['here'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s_test[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "            l2s_test[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s_test[index,7,:,0]=np.array(dataset[idx]['here'][1]['->'][:flow_size])/1000.0\n",
    "            labels_test[index]=0\n",
    "            index+=1\n",
    "\n",
    "        l2s_test[index,0,:,0]=np.array(dataset[i]['here'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s_test[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "        l2s_test[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s_test[index,3,:,0]=np.array(dataset[i]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "        l2s_test[index,4,:,0]=np.array(dataset[i]['here'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s_test[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "        l2s_test[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s_test[index,7,:,0]=np.array(dataset[i]['here'][1]['->'][:flow_size])/1000.0\n",
    "        #l2s_test[index,2,:,0]=dataset[i]['there'][0]['->'][:flow_size]\n",
    "        #l2s_test[index,3,:,0]=dataset[i]['here'][0]['<-'][:flow_size]\n",
    "\n",
    "        #l2s_test[index,0,:,1]=dataset[i]['here'][1]['->'][:flow_size]\n",
    "        #l2s_test[index,1,:,1]=dataset[i]['there'][1]['<-'][:flow_size]\n",
    "        #l2s_test[index,2,:,1]=dataset[i]['there'][1]['->'][:flow_size]\n",
    "        #l2s_test[index,3,:,1]=dataset[i]['here'][1]['<-'][:flow_size]\n",
    "        labels_test[index]=1\n",
    "\n",
    "        index+=1\n",
    "    return l2s, labels,l2s_test,labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(flow_before,dropout_keep_prob):\n",
    "    last_layer=flow_before\n",
    "    flat_layers_after=[flow_size*2,1000,50,1] #[600,1000,50,1]\n",
    "    for l in range(len(flat_layers_after)-1):\n",
    "        flat_weight = tf.get_variable(\"flat_after_weight%d\"%l, [flat_layers_after[l],flat_layers_after[l+1]],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.01,mean=0.0))\n",
    "\n",
    "        flat_bias = tf.get_variable(\"flat_after_bias%d\"%l, [flat_layers_after[l+1]],\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "        _x=tf.add(\n",
    "                tf.matmul(last_layer, flat_weight),\n",
    "                flat_bias)\n",
    "        if l<len(flat_layers_after)-2:\n",
    "            _x=tf.nn.dropout(tf.nn.relu(_x,name='relu_noise_flat_%d'%l),keep_prob=dropout_keep_prob)\n",
    "        last_layer=_x\n",
    "    return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn(flow_before,dropout_keep_prob):\n",
    "    last_layer=flow_before\n",
    "    \n",
    "    CNN_LAYERS=[[2,20,1,2000,5],[4,10,2000,800,3]]\n",
    "    \n",
    "    for cnn_size in range(len(CNN_LAYERS)):\n",
    "        cnn_weights = tf.get_variable(\"cnn_weight%d\"%cnn_size, CNN_LAYERS[cnn_size][:-1],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        cnn_bias = tf.get_variable(\"cnn_bias%d\"%cnn_size, [CNN_LAYERS[cnn_size][-2]],\n",
    "            initializer=tf.zeros_initializer())\n",
    "\n",
    "        _x = tf.nn.conv2d(last_layer, cnn_weights, strides=[1, 2,2, 1], padding='VALID')\n",
    "        _x = tf.nn.bias_add(_x, cnn_bias)\n",
    "        conv = tf.nn.relu(_x,name='relu_cnn_%d'%cnn_size)\n",
    "        pool = tf.nn.max_pool(conv, ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1], strides=[1, 1, 1, 1],padding='VALID')\n",
    "        last_layer=pool\n",
    "    last_layer=tf.reshape(last_layer, [batch_size,-1])\n",
    "    \n",
    "    flat_layers_after=[49600,3000,800,100,1]\n",
    "    for l in range(len(flat_layers_after)-1):\n",
    "        flat_weight = tf.get_variable(\"flat_after_weight%d\"%l, [flat_layers_after[l],flat_layers_after[l+1]],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.01,mean=0.0))\n",
    "\n",
    "        flat_bias = tf.get_variable(\"flat_after_bias%d\"%l, [flat_layers_after[l+1]],\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "        _x=tf.add(\n",
    "                tf.matmul(last_layer, flat_weight),\n",
    "                flat_bias)\n",
    "        if l<len(flat_layers_after)-2:\n",
    "            _x=tf.nn.dropout(tf.nn.relu(_x,name='relu_noise_flat_%d'%l),keep_prob=dropout_keep_prob)\n",
    "        last_layer=_x\n",
    "    return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    batch_size=64\n",
    "    learn_rate=0.0001\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():#8,flow_size,1表示height，width，inchannel\n",
    "        train_flow_before = tf.placeholder(tf.float32, shape=[batch_size, 8,flow_size,1],name='flow_before_placeholder')\n",
    "        train_label = tf.placeholder(tf.float32,name='label_placeholder',shape=[batch_size,1])\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32,name='dropout_placeholder')\n",
    "        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n",
    "        predict=tf.nn.sigmoid(y2)\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y2,labels=train_label),name='loss_sigmoid')\n",
    "        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "        s_loss=tf.summary.scalar('loss', loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "else:\n",
    "    batch_size=2804/2\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        train_flow_before = tf.placeholder(tf.float32, shape=[batch_size, 8,flow_size,1],name='flow_before_placeholder')\n",
    "        train_label = tf.placeholder(tf.float32,name='label_placeholder',shape=[batch_size,1])\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32,name='dropout_placeholder')\n",
    "        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n",
    "        predict=tf.nn.sigmoid(y2)\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=5\n",
    "import datetime\n",
    "\n",
    "writer = tf.summary.FileWriter('./logs', graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 10:53:14.785293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.785700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.786031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.786357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.824655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.825033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.825338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.825647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.825945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.826236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.826525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.826820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:14.827840: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 10:53:15.161583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.161957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.162268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.162568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.162873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.163161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.163449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.163736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.164046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.164336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.164626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:15.164910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.393255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.393642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.393978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.394299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.394600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.394899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.395192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.395484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.395779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.396089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22291 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2024-03-11 10:53:16.396562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.396844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22291 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:82:00.0, compute capability: 8.6\n",
      "2024-03-11 10:53:16.397115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.397390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22291 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2024-03-11 10:53:16.397634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-11 10:53:16.397913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22291 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c2:00.0, compute capability: 8.6\n",
      "100%|██████████| 1324/1324 [00:35<00:00, 36.93it/s]\n",
      "100%|██████████| 6000/6000 [02:44<00:00, 36.38it/s]\n",
      "2024-03-11 10:56:41.121675: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n",
      "2024-03-11 10:56:42.594212: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on validation set at step  0 :  0.7592377\n",
      "Average loss on validation set at step  100 :  0.064491\n",
      "Average loss on validation set at step  200 :  0.005175244\n",
      "Average loss on validation set at step  300 :  0.0026024308\n",
      "Average loss on validation set at step  400 :  0.00049904076\n",
      "Average loss on validation set at step  500 :  0.021806583\n",
      "Average loss on validation set at step  600 :  0.012871393\n",
      "Average loss on validation set at step  700 :  0.13550834\n",
      "Average loss on validation set at step  800 :  0.18328866\n",
      "Average loss on validation set at step  900 :  0.0049697654\n",
      "Average loss on validation set at step  1000 :  0.0056510633\n",
      "Average loss on validation set at step  1100 :  0.008553928\n",
      "Average loss on validation set at step  1200 :  0.013115223\n",
      "Average loss on validation set at step  1300 :  0.008238776\n",
      "Average loss on validation set at step  1400 :  0.23622552\n",
      "Average loss on validation set at step  1500 :  0.08384403\n",
      "Average loss on validation set at step  1600 :  0.09637883\n",
      "Average loss on validation set at step  1700 :  0.0024988698\n",
      "Average loss on validation set at step  1800 :  0.0032707276\n",
      "Average loss on validation set at step  1900 :  0.022390509\n",
      "Average loss on validation set at step  2000 :  0.0028248732\n",
      "Average loss on validation set at step  2100 :  0.00551234\n",
      "Average loss on validation set at step  2200 :  0.11352815\n",
      "Average loss on validation set at step  2300 :  0.0077233063\n",
      "Average loss on validation set at step  2400 :  0.11015192\n",
      "Average loss on validation set at step  2500 :  0.041041363\n",
      "Average loss on validation set at step  2600 :  0.116238646\n",
      "Average loss on validation set at step  2700 :  0.0008042177\n",
      "Average loss on validation set at step  2800 :  0.0853251\n",
      "Average loss on validation set at step  2900 :  0.0011783278\n",
      "Average loss on validation set at step  3000 :  0.0013518267\n",
      "Average loss on validation set at step  3100 :  0.005148354\n",
      "Average loss on validation set at step  3200 :  0.27321696\n",
      "Average loss on validation set at step  3300 :  0.011010248\n",
      "Average loss on validation set at step  3400 :  0.0012153127\n",
      "Average loss on validation set at step  3500 :  0.029438512\n",
      "Average loss on validation set at step  3600 :  0.0023232733\n",
      "Average loss on validation set at step  3700 :  0.21665874\n",
      "Average loss on validation set at step  3800 :  0.0056585236\n",
      "Average loss on validation set at step  3900 :  0.026813371\n",
      "Average loss on validation set at step  4000 :  0.0041175447\n",
      "Average loss on validation set at step  4100 :  0.005761843\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1324/1324 [00:35<00:00, 36.96it/s]\n",
      "100%|██████████| 6000/6000 [02:44<00:00, 36.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on validation set at step  4200 :  0.0022707982\n",
      "Average loss on validation set at step  4300 :  0.0012483889\n",
      "Average loss on validation set at step  4400 :  0.056115292\n",
      "Average loss on validation set at step  4500 :  0.023486136\n",
      "Average loss on validation set at step  4600 :  0.017043564\n",
      "Average loss on validation set at step  4700 :  0.007946796\n",
      "Average loss on validation set at step  4800 :  0.053300176\n",
      "Average loss on validation set at step  4900 :  0.042417914\n",
      "Average loss on validation set at step  5000 :  0.00390674\n",
      "Average loss on validation set at step  5100 :  0.0033593713\n",
      "Average loss on validation set at step  5200 :  0.008104741\n",
      "Average loss on validation set at step  5300 :  0.02377075\n",
      "Average loss on validation set at step  5400 :  0.06053863\n",
      "Average loss on validation set at step  5500 :  0.031651556\n",
      "Average loss on validation set at step  5600 :  0.023321664\n",
      "Average loss on validation set at step  5700 :  0.077181116\n",
      "Average loss on validation set at step  5800 :  0.006020165\n",
      "Average loss on validation set at step  5900 :  0.0024501223\n",
      "Average loss on validation set at step  6000 :  0.0013050335\n",
      "Average loss on validation set at step  6100 :  0.01032359\n",
      "Average loss on validation set at step  6200 :  0.009726659\n",
      "Average loss on validation set at step  6300 :  0.0023898147\n",
      "Average loss on validation set at step  6400 :  0.04870413\n",
      "Average loss on validation set at step  6500 :  0.006392157\n",
      "Average loss on validation set at step  6600 :  0.0050345613\n",
      "Average loss on validation set at step  6700 :  0.006800047\n",
      "Average loss on validation set at step  6800 :  0.0029002244\n",
      "Average loss on validation set at step  6900 :  0.02132776\n",
      "Average loss on validation set at step  7000 :  0.000711784\n",
      "Average loss on validation set at step  7100 :  0.0010310314\n",
      "Average loss on validation set at step  7200 :  0.0067744157\n",
      "Average loss on validation set at step  7300 :  0.038759984\n",
      "Average loss on validation set at step  7400 :  0.016740825\n",
      "Average loss on validation set at step  7500 :  0.0014207531\n",
      "Average loss on validation set at step  7600 :  0.0015060436\n",
      "Average loss on validation set at step  7700 :  0.010121592\n",
      "Average loss on validation set at step  7800 :  0.09937195\n",
      "Average loss on validation set at step  7900 :  0.0014116869\n",
      "Average loss on validation set at step  8000 :  0.008625555\n",
      "Average loss on validation set at step  8100 :  0.024778748\n",
      "Average loss on validation set at step  8200 :  0.021172725\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1324/1324 [00:35<00:00, 37.27it/s]\n",
      "100%|██████████| 6000/6000 [02:43<00:00, 36.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on validation set at step  8300 :  0.022228017\n",
      "Average loss on validation set at step  8400 :  0.0033733896\n",
      "Average loss on validation set at step  8500 :  0.0096315\n",
      "Average loss on validation set at step  8600 :  4.4895722e-07\n",
      "Average loss on validation set at step  8700 :  0.24946086\n",
      "Average loss on validation set at step  8800 :  0.00031287043\n",
      "Average loss on validation set at step  8900 :  0.002291417\n",
      "Average loss on validation set at step  9000 :  0.00044138537\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m     Y_est[start_ind:end_ind]\u001b[38;5;241m=\u001b[39mest\u001b[38;5;241m.\u001b[39mreshape((batch_size))\n\u001b[1;32m     75\u001b[0m num_samples_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(l2s_test)\u001b[38;5;241m/\u001b[39m(negetive_samples\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_samples_test\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     78\u001b[0m     best\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(Y_est[idx\u001b[38;5;241m*\u001b[39m(negetive_samples\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):(idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(negetive_samples\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels_test[best\u001b[38;5;241m+\u001b[39m(idx\u001b[38;5;241m*\u001b[39m(negetive_samples\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if TRAINING:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # We must initialize all variables before we use them.\n",
    "        session.run(init)\n",
    "        \n",
    "\n",
    "        for epoch in range(num_epochs ):\n",
    "            l2s,labels,l2s_test,labels_test=generate_data(dataset=dataset,train_index=train_index,test_index=test_index,flow_size=flow_size)\n",
    "            rr= list(range(len(l2s)))\n",
    "            np.random.shuffle(rr)\n",
    "            l2s=l2s[rr]\n",
    "            labels=labels[rr]\n",
    "\n",
    "\n",
    "            average_loss = 0\n",
    "            new_epoch=True\n",
    "            num_steps= (len(l2s)//batch_size)-1\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                start_ind = step*batch_size\n",
    "                end_ind = ((step + 1) *batch_size)\n",
    "                if end_ind < start_ind:\n",
    "                    print('HOOY')\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    batch_flow_before=l2s[start_ind:end_ind,:]\n",
    "                    batch_label= labels[start_ind:end_ind]\n",
    "\n",
    "\n",
    "                feed_dict = {train_flow_before: batch_flow_before,\n",
    "                                train_label:batch_label,\n",
    "                             dropout_keep_prob:0.6}\n",
    "                # We perform one update step by evaluating the optimizer op (including it\n",
    "                # in the list of returned values for session.run()\n",
    "\n",
    "                _, loss_val,summary = session.run([optimizer, loss, summary_op], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "\n",
    "                # average_loss += loss_val\n",
    "                writer.add_summary(summary, (epoch*num_steps) +step)\n",
    "\n",
    "                # print step, loss_val\n",
    "                # if step % FLAGS.print_every_n_steps == 0:\n",
    "                #     if step > 0:\n",
    "                #         average_loss /= FLAGS.print_every_n_steps\n",
    "                #     # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                #     print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                #     average_loss = 0.\n",
    "\n",
    "                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "\n",
    "                if ((epoch*num_steps) +step) % 100 == 0:\n",
    "                    print(\"Average loss on validation set at step \",  (epoch*num_steps) +step, \": \", loss_val)\n",
    "                if (((epoch*num_steps) +step)) % 3000 == 0 and epoch >1:\n",
    "                    tp=0\n",
    "                    fp=0\n",
    "\n",
    "                    num_steps_test= (len(l2s_test)//batch_size)-1\n",
    "                    Y_est=np.zeros((batch_size*(num_steps_test+1)))\n",
    "                    for step in range(num_steps_test):\n",
    "                        start_ind = step*batch_size\n",
    "                        end_ind = ((step + 1) *batch_size)\n",
    "                        test_batch_flow_before=l2s_test[start_ind:end_ind]\n",
    "                        feed_dict = {\n",
    "                                train_flow_before:test_batch_flow_before,\n",
    "                            dropout_keep_prob:1.0}\n",
    "\n",
    "\n",
    "                        est=session.run(predict, feed_dict=feed_dict)\n",
    "                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n",
    "                        Y_est[start_ind:end_ind]=est.reshape((batch_size))\n",
    "                    num_samples_test=int(len(l2s_test)/(negetive_samples+1))\n",
    "\n",
    "                    for idx in range(num_samples_test-1):\n",
    "                        best=np.argmax(Y_est[idx*(negetive_samples+1):(idx+1)*(negetive_samples+1)])\n",
    "\n",
    "                        if labels_test[best+(idx*(negetive_samples+1))]==1:\n",
    "                            tp+=1\n",
    "                        else:\n",
    "                            fp+=1\n",
    "                    print (tp,fp)\n",
    "                    acc= float(tp)/float(tp+fp)\n",
    "                    if float(tp)/float(tp+fp)>0.8:      \n",
    "                        print ('saving...')\n",
    "                        save_path = saver.save(session, \"./model/tor_199_epoch%d_step%d_acc%.2f.ckpt\"%(epoch,step,acc))\n",
    "                        print ('saved')\n",
    "            print ('Epoch',epoch)\n",
    "            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n",
    "\n",
    "            #t.join()\n",
    "else:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        name=input('model name')\n",
    "        saver.restore(session, \"/mnt/nfs/work1/amir/milad/%s\"%name)\n",
    "        print(\"Model restored.\")\n",
    "        corrs=np.zeros((len(test_index),len(test_index)))\n",
    "        batch=[]\n",
    "        l2s_test_all=np.zeros((batch_size,8,flow_size,1))\n",
    "        l_ids=[]\n",
    "        index=0\n",
    "        xi,xj=0,0\n",
    "        for i in tqdm.tqdm(test_index):\n",
    "            xj=0\n",
    "            for j in test_index:\n",
    "                \n",
    "                l2s_test_all[index,0,:,0]=np.array(dataset[j]['here'][0]['<-'][:flow_size])*1000.0\n",
    "                l2s_test_all[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "                l2s_test_all[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "                l2s_test_all[index,3,:,0]=np.array(dataset[j]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "                l2s_test_all[index,4,:,0]=np.array(dataset[j]['here'][1]['<-'][:flow_size])/1000.0\n",
    "                l2s_test_all[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "                l2s_test_all[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "                l2s_test_all[index,7,:,0]=np.array(dataset[j]['here'][1]['->'][:flow_size])/1000.0\n",
    "                l_ids.append((xi,xj))\n",
    "                index+=1\n",
    "                if index==batch_size:\n",
    "                    index=0\n",
    "                    cor_vals=session.run(predict,feed_dict={train_flow_before:l2s_test_all,\n",
    "                            dropout_keep_prob:1.0})\n",
    "                    for ids in range(len(l_ids)):\n",
    "                        di,dj=l_ids[ids]\n",
    "                        corrs[di,dj]=cor_vals[ids]\n",
    "                    l_ids=[]\n",
    "                xj+=1\n",
    "            xi+=1\n",
    "        np.save(open('correlation_values_test.np','w'),corrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
